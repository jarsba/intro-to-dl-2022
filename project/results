Training: Epoch 0 - Batch 0/438: Loss: 0.6867 | Train Acc: 55.804% (250/448)
Training: Epoch 0 - Batch 25/438: Loss: 0.0256 | Train Acc: 65.925% (7679/11648)
Training: Epoch 0 - Batch 50/438: Loss: 0.0125 | Train Acc: 75.853% (17331/22848)
Training: Epoch 0 - Batch 75/438: Loss: 0.0080 | Train Acc: 80.845% (27526/34048)
Training: Epoch 0 - Batch 100/438: Loss: 0.0056 | Train Acc: 83.518% (37790/45248)
Training: Epoch 0 - Batch 125/438: Loss: 0.0042 | Train Acc: 85.227% (48109/56448)
Training: Epoch 0 - Batch 150/438: Loss: 0.0033 | Train Acc: 86.436% (58472/67648)
Training: Epoch 0 - Batch 175/438: Loss: 0.0026 | Train Acc: 87.213% (68766/78848)
Training: Epoch 0 - Batch 200/438: Loss: 0.0021 | Train Acc: 87.978% (79222/90048)
Training: Epoch 0 - Batch 225/438: Loss: 0.0017 | Train Acc: 88.450% (89554/101248)
Training: Epoch 0 - Batch 250/438: Loss: 0.0015 | Train Acc: 88.873% (99936/112448)
Training: Epoch 0 - Batch 275/438: Loss: 0.0012 | Train Acc: 89.241% (110345/123648)
Training: Epoch 0 - Batch 300/438: Loss: 0.0011 | Train Acc: 89.524% (120721/134848)
Training: Epoch 0 - Batch 325/438: Loss: 0.0010 | Train Acc: 89.753% (131083/146048)
Training: Epoch 0 - Batch 350/438: Loss: 0.0009 | Train Acc: 89.962% (141463/157248)
Training: Epoch 0 - Batch 375/438: Loss: 0.0007 | Train Acc: 90.187% (151918/168448)
Training: Epoch 0 - Batch 400/438: Loss: 0.0008 | Train Acc: 90.348% (162309/179648)
Training: Epoch 0 - Batch 425/438: Loss: 0.0007 | Train Acc: 90.500% (172717/190848)
Validation: Epoch 0: Loss: 50.3248 | Validation Acc: 92.948% (78076/84000)
Training: Epoch 1 - Batch 0/438: Loss: 0.2561 | Train Acc: 93.527% (419/448)
Training: Epoch 1 - Batch 25/438: Loss: 0.0097 | Train Acc: 92.849% (10815/11648)
Training: Epoch 1 - Batch 50/438: Loss: 0.0052 | Train Acc: 92.529% (21141/22848)
Training: Epoch 1 - Batch 75/438: Loss: 0.0031 | Train Acc: 92.575% (31520/34048)
Training: Epoch 1 - Batch 100/438: Loss: 0.0026 | Train Acc: 92.506% (41857/45248)
Training: Epoch 1 - Batch 125/438: Loss: 0.0018 | Train Acc: 92.547% (52241/56448)
Training: Epoch 1 - Batch 150/438: Loss: 0.0018 | Train Acc: 92.590% (62635/67648)
Training: Epoch 1 - Batch 175/438: Loss: 0.0012 | Train Acc: 92.635% (73041/78848)
Training: Epoch 1 - Batch 200/438: Loss: 0.0011 | Train Acc: 92.665% (83443/90048)
Training: Epoch 1 - Batch 225/438: Loss: 0.0010 | Train Acc: 92.669% (93826/101248)
Training: Epoch 1 - Batch 250/438: Loss: 0.0009 | Train Acc: 92.676% (104212/112448)
Training: Epoch 1 - Batch 275/438: Loss: 0.0008 | Train Acc: 92.685% (114603/123648)
Training: Epoch 1 - Batch 300/438: Loss: 0.0008 | Train Acc: 92.664% (124956/134848)
Training: Epoch 1 - Batch 325/438: Loss: 0.0006 | Train Acc: 92.662% (135331/146048)
Training: Epoch 1 - Batch 350/438: Loss: 0.0007 | Train Acc: 92.667% (145717/157248)
Training: Epoch 1 - Batch 375/438: Loss: 0.0006 | Train Acc: 92.685% (156126/168448)
Training: Epoch 1 - Batch 400/438: Loss: 0.0006 | Train Acc: 92.689% (166514/179648)
Training: Epoch 1 - Batch 425/438: Loss: 0.0006 | Train Acc: 92.708% (176932/190848)
Validation: Epoch 1: Loss: 42.2266 | Validation Acc: 92.946% (78075/84000)
Training: Epoch 2 - Batch 0/438: Loss: 0.2187 | Train Acc: 93.527% (419/448)
Training: Epoch 2 - Batch 25/438: Loss: 0.0071 | Train Acc: 92.548% (10780/11648)
Training: Epoch 2 - Batch 50/438: Loss: 0.0041 | Train Acc: 92.271% (21082/22848)
Training: Epoch 2 - Batch 75/438: Loss: 0.0025 | Train Acc: 92.540% (31508/34048)
Training: Epoch 2 - Batch 100/438: Loss: 0.0024 | Train Acc: 92.570% (41886/45248)
Training: Epoch 2 - Batch 125/438: Loss: 0.0019 | Train Acc: 92.467% (52196/56448)
Training: Epoch 2 - Batch 150/438: Loss: 0.0017 | Train Acc: 92.600% (62642/67648)
Training: Epoch 2 - Batch 175/438: Loss: 0.0013 | Train Acc: 92.678% (73075/78848)
Training: Epoch 2 - Batch 200/438: Loss: 0.0011 | Train Acc: 92.762% (83530/90048)
Training: Epoch 2 - Batch 225/438: Loss: 0.0011 | Train Acc: 92.741% (93898/101248)
Training: Epoch 2 - Batch 250/438: Loss: 0.0008 | Train Acc: 92.753% (104299/112448)
Training: Epoch 2 - Batch 275/438: Loss: 0.0008 | Train Acc: 92.739% (114670/123648)
Training: Epoch 2 - Batch 300/438: Loss: 0.0007 | Train Acc: 92.729% (125043/134848)
Training: Epoch 2 - Batch 325/438: Loss: 0.0007 | Train Acc: 92.726% (135425/146048)
Training: Epoch 2 - Batch 350/438: Loss: 0.0006 | Train Acc: 92.720% (145801/157248)
Training: Epoch 2 - Batch 375/438: Loss: 0.0007 | Train Acc: 92.698% (156148/168448)
Training: Epoch 2 - Batch 400/438: Loss: 0.0005 | Train Acc: 92.698% (166530/179648)
Training: Epoch 2 - Batch 425/438: Loss: 0.0005 | Train Acc: 92.692% (176900/190848)
Validation: Epoch 2: Loss: 40.6848 | Validation Acc: 92.951% (78079/84000)
Training: Epoch 3 - Batch 0/438: Loss: 0.2574 | Train Acc: 90.402% (405/448)
Training: Epoch 3 - Batch 25/438: Loss: 0.0090 | Train Acc: 92.222% (10742/11648)
Training: Epoch 3 - Batch 50/438: Loss: 0.0042 | Train Acc: 92.179% (21061/22848)
Training: Epoch 3 - Batch 75/438: Loss: 0.0029 | Train Acc: 92.390% (31457/34048)
Training: Epoch 3 - Batch 100/438: Loss: 0.0020 | Train Acc: 92.424% (41820/45248)
Training: Epoch 3 - Batch 125/438: Loss: 0.0018 | Train Acc: 92.428% (52174/56448)
Training: Epoch 3 - Batch 150/438: Loss: 0.0012 | Train Acc: 92.496% (62572/67648)
Training: Epoch 3 - Batch 175/438: Loss: 0.0013 | Train Acc: 92.525% (72954/78848)
Training: Epoch 3 - Batch 200/438: Loss: 0.0010 | Train Acc: 92.524% (83316/90048)
Training: Epoch 3 - Batch 225/438: Loss: 0.0010 | Train Acc: 92.553% (93708/101248)
Training: Epoch 3 - Batch 250/438: Loss: 0.0007 | Train Acc: 92.579% (104103/112448)
Training: Epoch 3 - Batch 275/438: Loss: 0.0008 | Train Acc: 92.614% (114515/123648)
Training: Epoch 3 - Batch 300/438: Loss: 0.0006 | Train Acc: 92.672% (124967/134848)
Training: Epoch 3 - Batch 325/438: Loss: 0.0006 | Train Acc: 92.693% (135376/146048)
Training: Epoch 3 - Batch 350/438: Loss: 0.0005 | Train Acc: 92.704% (145775/157248)
Training: Epoch 3 - Batch 375/438: Loss: 0.0006 | Train Acc: 92.712% (156172/168448)
Training: Epoch 3 - Batch 400/438: Loss: 0.0006 | Train Acc: 92.720% (166569/179648)
Training: Epoch 3 - Batch 425/438: Loss: 0.0005 | Train Acc: 92.707% (176930/190848)
Validation: Epoch 3: Loss: 40.1741 | Validation Acc: 92.946% (78075/84000)
Training: Epoch 4 - Batch 0/438: Loss: 0.2482 | Train Acc: 91.071% (408/448)
Training: Epoch 4 - Batch 25/438: Loss: 0.0091 | Train Acc: 92.969% (10829/11648)
Training: Epoch 4 - Batch 50/438: Loss: 0.0041 | Train Acc: 92.984% (21245/22848)
Training: Epoch 4 - Batch 75/438: Loss: 0.0029 | Train Acc: 92.804% (31598/34048)
Training: Epoch 4 - Batch 100/438: Loss: 0.0020 | Train Acc: 92.842% (42009/45248)
Training: Epoch 4 - Batch 125/438: Loss: 0.0018 | Train Acc: 92.790% (52378/56448)
Training: Epoch 4 - Batch 150/438: Loss: 0.0014 | Train Acc: 92.767% (62755/67648)
Training: Epoch 4 - Batch 175/438: Loss: 0.0013 | Train Acc: 92.748% (73130/78848)
Training: Epoch 4 - Batch 200/438: Loss: 0.0011 | Train Acc: 92.713% (83486/90048)
Training: Epoch 4 - Batch 225/438: Loss: 0.0009 | Train Acc: 92.690% (93847/101248)
Training: Epoch 4 - Batch 250/438: Loss: 0.0010 | Train Acc: 92.702% (104241/112448)
Training: Epoch 4 - Batch 275/438: Loss: 0.0008 | Train Acc: 92.727% (114655/123648)
Training: Epoch 4 - Batch 300/438: Loss: 0.0006 | Train Acc: 92.748% (125069/134848)
Training: Epoch 4 - Batch 325/438: Loss: 0.0005 | Train Acc: 92.728% (135428/146048)
Training: Epoch 4 - Batch 350/438: Loss: 0.0006 | Train Acc: 92.713% (145789/157248)
Training: Epoch 4 - Batch 375/438: Loss: 0.0005 | Train Acc: 92.708% (156164/168448)
Training: Epoch 4 - Batch 400/438: Loss: 0.0006 | Train Acc: 92.694% (166523/179648)
Training: Epoch 4 - Batch 425/438: Loss: 0.0005 | Train Acc: 92.704% (176924/190848)
Validation: Epoch 4: Loss: 40.0275 | Validation Acc: 92.946% (78075/84000)
Training: Epoch 5 - Batch 0/438: Loss: 0.2532 | Train Acc: 92.188% (413/448)
Training: Epoch 5 - Batch 25/438: Loss: 0.0074 | Train Acc: 92.531% (10778/11648)
Training: Epoch 5 - Batch 50/438: Loss: 0.0046 | Train Acc: 92.752% (21192/22848)
Training: Epoch 5 - Batch 75/438: Loss: 0.0027 | Train Acc: 92.699% (31562/34048)
Training: Epoch 5 - Batch 100/438: Loss: 0.0021 | Train Acc: 92.649% (41922/45248)
Training: Epoch 5 - Batch 125/438: Loss: 0.0014 | Train Acc: 92.673% (52312/56448)
Training: Epoch 5 - Batch 150/438: Loss: 0.0015 | Train Acc: 92.677% (62694/67648)
Training: Epoch 5 - Batch 175/438: Loss: 0.0012 | Train Acc: 92.662% (73062/78848)
Training: Epoch 5 - Batch 200/438: Loss: 0.0012 | Train Acc: 92.707% (83481/90048)
Training: Epoch 5 - Batch 225/438: Loss: 0.0011 | Train Acc: 92.683% (93840/101248)
Training: Epoch 5 - Batch 250/438: Loss: 0.0007 | Train Acc: 92.655% (104189/112448)
Training: Epoch 5 - Batch 275/438: Loss: 0.0009 | Train Acc: 92.702% (114624/123648)
Training: Epoch 5 - Batch 300/438: Loss: 0.0007 | Train Acc: 92.712% (125020/134848)
Training: Epoch 5 - Batch 325/438: Loss: 0.0007 | Train Acc: 92.701% (135388/146048)
Training: Epoch 5 - Batch 350/438: Loss: 0.0008 | Train Acc: 92.717% (145795/157248)
Training: Epoch 5 - Batch 375/438: Loss: 0.0006 | Train Acc: 92.717% (156180/168448)
Training: Epoch 5 - Batch 400/438: Loss: 0.0006 | Train Acc: 92.705% (166543/179648)
Training: Epoch 5 - Batch 425/438: Loss: 0.0006 | Train Acc: 92.710% (176936/190848)
Validation: Epoch 5: Loss: 39.8822 | Validation Acc: 92.949% (78077/84000)
Training: Epoch 6 - Batch 0/438: Loss: 0.2199 | Train Acc: 92.188% (413/448)
Training: Epoch 6 - Batch 25/438: Loss: 0.0077 | Train Acc: 92.720% (10800/11648)
Training: Epoch 6 - Batch 50/438: Loss: 0.0038 | Train Acc: 92.743% (21190/22848)
Training: Epoch 6 - Batch 75/438: Loss: 0.0023 | Train Acc: 92.713% (31567/34048)
