{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n   Introduction to Deep Learning\\n   Assignment 3: Sentiment Classification of Tweets on a Recurrent Neural Network using Pretrained Embeddings\\n\\n   Hande Celikkanat\\n\\n   Credit: Data preparation pipeline adopted from https://medium.com/@sonicboom8/sentiment-analysis-torchtext-55fb57b1fab8\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "   Introduction to Deep Learning\n",
    "   Assignment 3: Sentiment Classification of Tweets on a Recurrent Neural Network using Pretrained Embeddings\n",
    "\n",
    "   Hande Celikkanat\n",
    "\n",
    "   Credit: Data preparation pipeline adopted from https://medium.com/@sonicboom8/sentiment-analysis-torchtext-55fb57b1fab8\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import spacy\n",
    "import regex as re\n",
    "from torchtext.legacy import vocab\n",
    "from torchtext.legacy import data\n",
    "import time\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants - Add here as you wish\n",
    "BATCH_SIZE = 50\n",
    "N_EPOCHS = 50\n",
    "EMBEDDING_DIM = 200\n",
    "OUTPUT_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxilary functions for data preparation\n",
    "tok = spacy.load('en_core_web_sm',disable=['parser', 'tagger', 'ner'])\n",
    "def tokenizer(s):\n",
    "    return [w.text.lower() for w in tok(tweet_clean(s))]\n",
    "\n",
    "def tweet_clean(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric character\n",
    "    text = re.sub(r'https?:/\\/\\S+', ' ', text) # remove links\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "def get_accuracy(output, gold):\n",
    "    predictions = (output >= 0.5).type(torch.uint8)\n",
    "    correct = torch.sum(torch.eq(predictions, gold)).item()\n",
    "    acc = correct / gold.shape[0]\n",
    "    return acc\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            \n",
    "            texts, text_lengths = batch.TweetText\n",
    "            texts_T = texts.transpose(0,1)\n",
    "            predictions = model(texts_T, text_lengths).squeeze(1)\n",
    "            labels = batch.Label\n",
    "            loss = criterion(predictions.float(), labels.float())\n",
    "\n",
    "            acc = get_accuracy(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_length, embedding_dim, hidden_size, num_hidden_layers):\n",
    "        super().__init__()\n",
    "        self.embedding_length = embedding_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.embedding = nn.Embedding(num_embeddings=embedding_length, embedding_dim=EMBEDDING_DIM)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embedding_dim, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_hidden_layers, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc1 = nn.Linear(2 * hidden_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 100)\n",
    "        self.fc3 = nn.Linear(100,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, texts, text_lengths: torchtext.legacy.data.Field):\n",
    "        x = self.embedding(texts)\n",
    "        packed_embedded = pack_padded_sequence(x, text_lengths.cpu(), batch_first=True)\n",
    "        packed_output, _ = self.rnn(packed_embedded)\n",
    "        output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        out_forward = output[range(len(output)), text_lengths - 1, : self.hidden_size]\n",
    "        out_reverse = output[:, 0, self.hidden_size : ]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        \n",
    "        # output = torch.squeeze(h_t)\n",
    "        out = self.fc1(out_reduced)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarsba/venvs/basic-pytorch/lib/python3.8/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "# --- Data Preparation ---\n",
    "\n",
    "# define the columns that we want to process and how to process\n",
    "txt_field = torchtext.legacy.data.Field(sequential=True,\n",
    "                                 tokenize=tokenizer,\n",
    "                                 include_lengths=True,\n",
    "                                 use_vocab=True)\n",
    "label_field = torchtext.legacy.data.Field(sequential=False,\n",
    "                                   use_vocab=False)\n",
    "\n",
    "csv_fields = [\n",
    "    ('Label', label_field), # process this field as the class label\n",
    "    ('TweetID', None), # we dont need this field\n",
    "    ('Timestamp', None), # we dont need this field\n",
    "    ('Flag', None), # we dont need this field\n",
    "    ('UseerID', None), # we dont need this field\n",
    "    ('TweetText', txt_field) # process it as text field\n",
    "]\n",
    "\n",
    "train_data, dev_data, test_data = torchtext.legacy.data.TabularDataset.splits(path='../data',\n",
    "                                                                       format='csv',\n",
    "                                                                       train='sent140.train.mini.csv',\n",
    "                                                                       validation='sent140.dev.csv',\n",
    "                                                                       test='sent140.test.csv',\n",
    "                                                                       fields=csv_fields,\n",
    "                                                                       skip_header=False)\n",
    "\n",
    "\n",
    "txt_field.build_vocab(\n",
    "    train_data, \n",
    "    dev_data,\n",
    "    max_size=100000, \n",
    "    vectors='glove.twitter.27B.200d', \n",
    "    unk_init = torch.Tensor.normal_\n",
    ")\n",
    "\n",
    "label_field.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, dev_iter, test_iter = torchtext.legacy.data.BucketIterator.splits(datasets=(train_data, dev_data, test_data),\n",
    "                                            batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),  # batch sizes of train, dev, test\n",
    "                                            sort_key=lambda x: len(x.TweetText), # how to sort text\n",
    "                                            device=device,\n",
    "                                            sort_within_batch=True,\n",
    "                                            repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model, Loss, Optimizer Initialization ---\n",
    "\n",
    "PAD_IDX = txt_field.vocab.stoi[txt_field.pad_token]\n",
    "UNK_IDX = txt_field.vocab.stoi[txt_field.unk_token]\n",
    "\n",
    "# WRITE CODE HERE\n",
    "HIDDEN_SIZE = 50\n",
    "NUM_LAYERS_HIDDEN = 8\n",
    "\n",
    "model = RNN(\n",
    "    embedding_length=len(txt_field.vocab), \n",
    "    embedding_dim=EMBEDDING_DIM, \n",
    "    hidden_size=HIDDEN_SIZE, \n",
    "    num_hidden_layers=NUM_LAYERS_HIDDEN\n",
    ")\n",
    "\n",
    "# Copy the pretrained embeddings into the model\n",
    "pretrained_embeddings = txt_field.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "# Fix the <UNK> and <PAD> tokens in the embedding layer\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "# WRITE CODE HERE\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.618 | Train Acc: 64.61%\n",
      "\t Val. Loss: 0.543 |  Val. Acc: 72.26%\n",
      "Epoch: 02 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.526 | Train Acc: 73.90%\n",
      "\t Val. Loss: 0.531 |  Val. Acc: 73.35%\n",
      "Epoch: 03 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.479 | Train Acc: 77.15%\n",
      "\t Val. Loss: 0.508 |  Val. Acc: 75.39%\n",
      "Epoch: 04 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.446 | Train Acc: 79.26%\n",
      "\t Val. Loss: 0.499 |  Val. Acc: 75.49%\n",
      "Epoch: 05 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.425 | Train Acc: 80.76%\n",
      "\t Val. Loss: 0.497 |  Val. Acc: 76.03%\n",
      "Epoch: 06 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.388 | Train Acc: 83.22%\n",
      "\t Val. Loss: 0.512 |  Val. Acc: 75.69%\n",
      "Epoch: 07 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.363 | Train Acc: 84.77%\n",
      "\t Val. Loss: 0.527 |  Val. Acc: 75.31%\n",
      "Epoch: 08 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.334 | Train Acc: 86.30%\n",
      "\t Val. Loss: 0.550 |  Val. Acc: 74.58%\n",
      "Epoch: 09 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.305 | Train Acc: 87.53%\n",
      "\t Val. Loss: 0.583 |  Val. Acc: 74.73%\n",
      "Epoch: 10 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.282 | Train Acc: 88.72%\n",
      "\t Val. Loss: 0.627 |  Val. Acc: 75.01%\n",
      "Epoch: 11 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.243 | Train Acc: 90.61%\n",
      "\t Val. Loss: 0.675 |  Val. Acc: 74.83%\n",
      "Epoch: 12 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.212 | Train Acc: 91.78%\n",
      "\t Val. Loss: 0.738 |  Val. Acc: 73.79%\n",
      "Epoch: 13 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.183 | Train Acc: 93.61%\n",
      "\t Val. Loss: 0.780 |  Val. Acc: 74.00%\n",
      "Epoch: 14 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.154 | Train Acc: 94.36%\n",
      "\t Val. Loss: 0.930 |  Val. Acc: 74.14%\n",
      "Epoch: 15 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.122 | Train Acc: 95.55%\n",
      "\t Val. Loss: 1.021 |  Val. Acc: 72.47%\n",
      "Epoch: 16 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.097 | Train Acc: 96.68%\n",
      "\t Val. Loss: 1.154 |  Val. Acc: 72.53%\n",
      "Epoch: 17 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.065 | Train Acc: 97.99%\n",
      "\t Val. Loss: 1.509 |  Val. Acc: 72.87%\n",
      "Epoch: 18 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.049 | Train Acc: 98.47%\n",
      "\t Val. Loss: 1.774 |  Val. Acc: 72.20%\n",
      "Epoch: 19 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.71%\n",
      "\t Val. Loss: 2.364 |  Val. Acc: 71.63%\n",
      "Epoch: 20 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.027 | Train Acc: 99.14%\n",
      "\t Val. Loss: 3.042 |  Val. Acc: 71.91%\n",
      "Epoch: 21 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.029 | Train Acc: 98.99%\n",
      "\t Val. Loss: 3.250 |  Val. Acc: 71.55%\n",
      "Epoch: 22 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.63%\n",
      "\t Val. Loss: 4.080 |  Val. Acc: 71.47%\n",
      "Epoch: 23 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.006 | Train Acc: 99.90%\n",
      "\t Val. Loss: 4.592 |  Val. Acc: 71.21%\n",
      "Epoch: 24 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.003 | Train Acc: 99.96%\n",
      "\t Val. Loss: 5.355 |  Val. Acc: 71.72%\n",
      "Epoch: 25 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.003 | Train Acc: 99.93%\n",
      "\t Val. Loss: 5.538 |  Val. Acc: 71.28%\n",
      "Epoch: 26 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.006 | Train Acc: 99.90%\n",
      "\t Val. Loss: 6.277 |  Val. Acc: 71.76%\n",
      "Epoch: 27 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.002 | Train Acc: 99.96%\n",
      "\t Val. Loss: 6.351 |  Val. Acc: 71.52%\n",
      "Epoch: 28 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 6.803 |  Val. Acc: 71.61%\n",
      "Epoch: 29 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 7.032 |  Val. Acc: 71.48%\n",
      "Epoch: 30 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 7.253 |  Val. Acc: 71.79%\n",
      "Epoch: 31 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 7.288 |  Val. Acc: 71.54%\n",
      "Epoch: 32 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 7.665 |  Val. Acc: 71.74%\n",
      "Epoch: 33 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 7.558 |  Val. Acc: 71.55%\n",
      "Epoch: 34 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 7.634 |  Val. Acc: 71.49%\n",
      "Epoch: 35 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 8.167 |  Val. Acc: 71.78%\n",
      "Epoch: 36 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 7.881 |  Val. Acc: 71.55%\n",
      "Epoch: 37 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 8.219 |  Val. Acc: 71.67%\n",
      "Epoch: 38 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 8.372 |  Val. Acc: 71.71%\n",
      "Epoch: 39 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 8.439 |  Val. Acc: 71.69%\n",
      "Epoch: 40 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 8.567 |  Val. Acc: 71.72%\n",
      "Epoch: 41 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 8.658 |  Val. Acc: 71.68%\n",
      "Epoch: 42 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 8.774 |  Val. Acc: 71.67%\n",
      "Epoch: 43 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 8.808 |  Val. Acc: 71.70%\n",
      "Epoch: 44 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 8.971 |  Val. Acc: 71.65%\n",
      "Epoch: 45 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 9.130 |  Val. Acc: 71.71%\n",
      "Epoch: 46 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 9.116 |  Val. Acc: 71.68%\n",
      "Epoch: 47 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 9.188 |  Val. Acc: 71.64%\n",
      "Epoch: 48 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 9.282 |  Val. Acc: 71.62%\n",
      "Epoch: 49 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 9.446 |  Val. Acc: 71.68%\n",
      "Epoch: 50 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 9.467 |  Val. Acc: 71.60%\n"
     ]
    }
   ],
   "source": [
    "# --- Train Loop ---\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        optimizer.zero_grad()\n",
    "        texts, text_lengths = batch.TweetText\n",
    "        # Switch batch_size to first\n",
    "        texts_T = texts.transpose(0,1)\n",
    "        predictions = model(texts_T, text_lengths).squeeze(1)\n",
    "        \n",
    "        labels = batch.Label\n",
    "        loss = criterion(predictions.float(), labels.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().cpu()\n",
    "        epoch_acc += get_accuracy(predictions, labels)\n",
    "    \n",
    "\n",
    "    train_loss, train_acc = (epoch_loss / len(train_iter), epoch_acc / len(train_iter))\n",
    "    valid_loss, valid_acc = evaluate(model, dev_iter, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic-pytorch",
   "language": "python",
   "name": "basic-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
